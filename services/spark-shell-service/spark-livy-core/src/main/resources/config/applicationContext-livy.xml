<?xml version = "1.0" encoding = "UTF-8"?>
<!--
  #%L
  kylo-spark-livy-core
  %%
  Copyright (C) 2017 - 2018 ThinkBig Analytics, a Teradata Company
  %%
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
  #L%
-->


<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
           http://www.springframework.org/schema/beans/spring-beans-4.2.xsd">


    <bean id="scriptRegistry" class="java.util.HashMap">
        <constructor-arg>
            <map key-type="java.lang.String" value-type="java.lang.String">
                <entry key="initSession">
                    <value>
                        <![CDATA[
val ctx = com.thinkbiganalytics.spark.LivyWrangler.createSpringContext(sc, sqlContext)
val profiler = ctx.getBean(classOf[com.thinkbiganalytics.spark.dataprofiler.Profiler])
val transformService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.TransformService])
val sparkContextService = ctx.getBean(classOf[com.thinkbiganalytics.spark.SparkContextService])
val converterService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.DataSetConverterService])
val sparkShellTransformController =
ctx.getBean(classOf[com.thinkbiganalytics.spark.rest.SparkShellTransformController])
val sparkUtilityService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.SparkUtilityService])
val mapper = new com.fasterxml.jackson.databind.ObjectMapper()
mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)

import org.apache.log4j.{Level, Logger}
val logger = Logger.getLogger("com.thinkbiganalytics.spark.logger.LivyLogger")
                        ]]>
                    </value>
                </entry>
                <entry key="newSaveRequest">
                    <value>
                        <![CDATA[                        
val saveRequest: com.thinkbiganalytics.spark.rest.model.SaveRequest = new
com.thinkbiganalytics.spark.rest.model.SaveRequest()
saveRequest.setFormat(%s)
saveRequest.setJdbc(null)
saveRequest.setMode(%s)
saveRequest.setOptions(new java.util.HashMap[String,String])
saveRequest.setTableName(%s)
                        ]]>
                    </value>
                </entry>
                <entry key="submitSaveJob">
                    <value>
                        <![CDATA[
val dataSet = sparkContextService.toDataSet(sqlContext.sql("select * from %s"))
val saveResponse = transformService.submitSaveJob(transformService.createSaveTask(saveRequest,
    new com.thinkbiganalytics.spark.metadata.ShellTransformStage(dataSet, converterService)))
val respAsStr = mapper.writeValueAsString(saveResponse)
%%json respAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="getSave">
                    <value>
                        <![CDATA[
val response = sparkShellTransformController.getSave("%s")
val saveResponse =
response.getEntity().asInstanceOf[com.thinkbiganalytics.spark.rest.model.SaveResponse]
val respAsStr = mapper.writeValueAsString(saveResponse)
%%json respAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="profileDataFrame">
                    <value>
                        <![CDATA[
val dfProf =
com.thinkbiganalytics.spark.dataprofiler.core.Profiler.profileDataFrame(sparkContextService,profiler,df)
%%json dfProf
                        ]]>
                    </value>
                </entry>


                <entry key="pagedDataFrame">
                    <value>
                        <![CDATA[
var dfRows : List[Object] = List()
LivyLogger.time {
  val (startCol, stopCol) = (%s, %s);
  val lastCol = df.columns.length - 1
  val dfStartCol = if( lastCol >= startCol ) startCol else lastCol
  val dfStopCol = if( lastCol >= stopCol) stopCol else lastCol
  df = df.select( dfStartCol to dfStopCol map df.columns map col: _*)

  val dl = df.collect
  val ( firstRow, lastRow ) = ( 0, dl.size )
  val ( pageStart, pageStop ) = ( %s, %s )
  val dfStartRow = if( lastRow >= pageStart ) pageStart else lastRow
  val dfStopRow = if( lastRow >= pageStop) pageStop else lastRow

  val pagedRows = dl.slice( dfStartRow, dfStopRow ).map( _.toSeq )
  dfRows = List( df.schema.json, pagedRows )
}
                        ]]>
                    </value>
                </entry>


                <entry key="OLDpagedDataFrame">
                    <value>
                        <![CDATA[
LivyLogger.time {
  val (startCol, stopCol) = (%s, %s);
  val lastCol = df.columns.length - 1
  val dfStartCol = if( lastCol >= startCol ) startCol else lastCol
  val dfStopCol = if( lastCol >= stopCol) stopCol else lastCol
  df = df.select( dfStartCol to dfStopCol map df.columns map col: _*)
}

var dz : org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Long)] = null;
LivyLogger.time {
   dz = df.rdd.zipWithIndex
}

var dfRows : List[Object] = List()
LivyLogger.time {
  dfRows = List( df.schema.json, dz.filter( pair => pair._2>= %s  && pair._2<= %s)
    .map(_._1).collect.map(x => x.toSeq) )
}
                        ]]>
                    </value>
                </entry>

                <entry key="getDataSources">
                    <value>
                        <![CDATA[
val dataSources = sparkUtilityService.getDataSources()

val respAsStr = mapper.writeValueAsString(dataSources)
%%json respAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="getSaveResult">
                    <value>
                        <![CDATA[
val saveResult = sparkShellTransformController.getSaveResult(%s)
val respAsStr = mapper.writeValueAsString(saveResult.getPath().toUri())
%%json respAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="kyloCatalogTransform">
                    <value> <!-- takes a JSON string escaped for scala -->
            <![CDATA[
val kyloCatalogReadRequest: com.thinkbiganalytics.spark.rest.model.KyloCatalogReadRequest = mapper.readValue(%s, classOf[com.thinkbiganalytics.spark.rest.model.KyloCatalogReadRequest]);
val kyloReadResponse = transformService.kyloReaderResponse(kyloCatalogReadRequest);
val respAsStr = mapper.writeValueAsString(kyloReadResponse)
%%json respAsStr
]]>
                    </value>
                </entry>
            </map>
        </constructor-arg>
    </bean>

</beans>