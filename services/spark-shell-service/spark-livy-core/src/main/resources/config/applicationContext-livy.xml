<?xml version = "1.0" encoding = "UTF-8"?>
<!--
  #%L
  kylo-spark-livy-core
  %%
  Copyright (C) 2017 - 2018 ThinkBig Analytics, a Teradata Company
  %%
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
  #L%
-->


<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
           http://www.springframework.org/schema/beans/spring-beans-4.2.xsd">

  <bean id="scriptRegistry" class="java.util.HashMap">
    <constructor-arg>
      <map key-type="java.lang.String" value-type="java.lang.String">
        <entry key="initSession">
          <value>
            <![CDATA[
val sqlContext = org.apache.spark.sql.SQLContext.getOrCreate(sc)
val ctx = com.thinkbiganalytics.spark.LivyWrangler.createSpringContext(sc, sqlContext)
val profiler = ctx.getBean(classOf[com.thinkbiganalytics.spark.dataprofiler.Profiler])
val transformService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.TransformService])
val sparkContextService = ctx.getBean(classOf[com.thinkbiganalytics.spark.SparkContextService])
val converterService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.DataSetConverterService])
val sparkShellTransformController =
ctx.getBean(classOf[com.thinkbiganalytics.spark.rest.SparkShellTransformController])
val sparkUtilityService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.SparkUtilityService])
val mapper = new com.fasterxml.jackson.databind.ObjectMapper()
mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)

import org.slf4j.LoggerFactory
import com.thinkbiganalytics.spark.logger.LivyLogger
val logger = Logger.getLogger("com.thinkbiganalytics.spark.logger.LivyLogger")
                        ]]>
          </value>
        </entry>
        <entry key="submitSaveJob">
          <value>
            <![CDATA[
val saveRequest: com.thinkbiganalytics.spark.rest.model.SaveRequest = mapper.readValue(%s, classOf[com.thinkbiganalytics.spark.rest.model.SaveRequest]);
val dataSet = sparkContextService.toDataSet(sqlContext.sql("select * from %s"))
val saveResponse = transformService.submitSaveJob(transformService.createSaveTask(saveRequest,
    new com.thinkbiganalytics.spark.metadata.ShellTransformStage(dataSet, converterService)))
val saveResponseAsStr = mapper.writeValueAsString(saveResponse)
%%json saveResponseAsStr
                        ]]>
          </value>
        </entry>
        <entry key="getSave">
          <value>
            <![CDATA[
val response = sparkShellTransformController.getSave("%s")
val saveResponse =
response.getEntity().asInstanceOf[com.thinkbiganalytics.spark.rest.model.SaveResponse]
val saveResponseAsStr = mapper.writeValueAsString(saveResponse)
%%json saveResponseAsStr
                        ]]>
          </value>
        </entry>
        <entry key="profileDataFrame">
          <value>
            <![CDATA[
val dfProf =
com.thinkbiganalytics.spark.dataprofiler.core.Profiler.profileDataFrame(sparkContextService,profiler,df)
%%json dfProf
                        ]]>
          </value>
        </entry>


        <entry key="pagedDataFrame">
          <value>
            <![CDATA[
var dfRows : List[Object] = List()
LivyLogger.time {
  val (startCol, stopCol) = (%s, %s);
  val lastCol = df.columns.length - 1
  val dfStartCol = if( lastCol >= startCol ) startCol else lastCol
  val dfStopCol = if( lastCol >= stopCol) stopCol else lastCol
  df = df.select( dfStartCol to dfStopCol map df.columns map col: _*)

  val dl = df.collect
  val ( firstRow, lastRow ) = ( 0, dl.size )
  val ( pageStart, pageStop ) = ( %s, %s )
  val dfStartRow = if( lastRow >= pageStart ) pageStart else lastRow
  val dfStopRow = if( lastRow >= pageStop) pageStop else lastRow

  val pagedRows = dl.slice( dfStartRow, dfStopRow ).map( _.toSeq )
  dfRows = List( df.schema.json, pagedRows )
}
                        ]]>
          </value>
        </entry>

        <entry key="getDataSources">
          <value>
            <![CDATA[
val dataSources = sparkUtilityService.getDataSources()

val dataSourcesAsStr = mapper.writeValueAsString(dataSources)
%%json dataSourcesAsStr
                        ]]>
          </value>
        </entry>
        <entry key="getSaveResult">
          <value>
            <![CDATA[
val saveResult = sparkShellTransformController.getSaveResult(%s)
val saveResultAsStr = mapper.writeValueAsString(saveResult.getPath().toUri())
%%json saveResultAsStr
                        ]]>
          </value>
        </entry>
        <entry key="kyloCatalogTransform">
          <value> <!-- takes a JSON string escaped for scala -->
            <![CDATA[
val kyloCatalogReadRequest: com.thinkbiganalytics.spark.rest.model.KyloCatalogReadRequest = mapper.readValue(%s, classOf[com.thinkbiganalytics.spark.rest.model.KyloCatalogReadRequest]);
val kyloReadResponse = transformService.kyloReaderResponse(kyloCatalogReadRequest);
val transformAsStr = mapper.writeValueAsString(kyloReadResponse)
%%json transformAsStr
]]>
          </value>
        </entry>
        <entry key="getTransform">
          <value> <!-- takes a JSON string escaped for scala -->
            <![CDATA[
val response = sparkShellTransformController.getTableTransformResponse(%s)
val transformAsStr = mapper.writeValueAsString(response)
%%json transformAsStr
]]>
          </value>
        </entry>
        <entry key="readTable">
          <value> <!-- first parameter takes a JSON string escaped for scala, second parameter is valid scala -->
            <![CDATA[
var tbl :String = null;
var parent = try {
  tbl = %s;
  sqlContext.read.table(tbl)
  logger.info("Parent dataframe set to table '{}'", tbl);
} catch { case _ : Exception =>  {
  logger.info("Unable to load data from parent table '{}'.  Rebuild from transformation history.", tbl);
%s
}}
]]>
          </value>
        </entry>
      </map>
    </constructor-arg>
  </bean>
</beans>