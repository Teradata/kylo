<?xml version = "1.0" encoding = "UTF-8"?>
<!--
  #%L
  kylo-spark-livy-core
  %%
  Copyright (C) 2017 - 2018 ThinkBig Analytics, a Teradata Company
  %%
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
  #L%
-->


<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
           http://www.springframework.org/schema/beans/spring-beans-4.2.xsd">


    <bean id="scriptRegistry" class="java.util.HashMap">
        <constructor-arg>
            <map key-type="java.lang.String" value-type="java.lang.String">
                <entry key="initSession">
                    <value>
                        <![CDATA[
val ctx = com.thinkbiganalytics.spark.LivyWrangler.createSpringContext(sc, sqlContext)
val profiler = ctx.getBean(classOf[com.thinkbiganalytics.spark.dataprofiler.Profiler])
val transformService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.TransformService])
val sparkContextService = ctx.getBean(classOf[com.thinkbiganalytics.spark.SparkContextService])
val converterService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.DataSetConverterService])
val sparkShellTransformController =
ctx.getBean(classOf[com.thinkbiganalytics.spark.rest.SparkShellTransformController])
val sparkUtilityService =
ctx.getBean(classOf[com.thinkbiganalytics.spark.service.SparkUtilityService])
val mapper = new com.fasterxml.jackson.databind.ObjectMapper()
mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)

import org.apache.log4j.{Level, Logger}
import com.thinkbiganalytics.spark.logger.LivyLogger
val logger = Logger.getLogger("com.thinkbiganalytics.spark.logger.LivyLogger")
                        ]]>
                    </value>
                </entry>
                <entry key="submitSaveJob">
                    <value>
                        <![CDATA[
val saveRequest: com.thinkbiganalytics.spark.rest.model.SaveRequest = mapper.readValue(%s, classOf[com.thinkbiganalytics.spark.rest.model.SaveRequest]);
val dataSet = sparkContextService.toDataSet(sqlContext.sql("select * from %s"))
val saveResponse = transformService.submitSaveJob(transformService.createSaveTask(saveRequest,
    new com.thinkbiganalytics.spark.metadata.ShellTransformStage(dataSet, converterService)))
val saveResponseAsStr = mapper.writeValueAsString(saveResponse)
%%json saveResponseAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="getSave">
                    <value>
                        <![CDATA[
val response = sparkShellTransformController.getSave("%s")
val saveResponse =
response.getEntity().asInstanceOf[com.thinkbiganalytics.spark.rest.model.SaveResponse]
val saveResponseAsStr = mapper.writeValueAsString(saveResponse)
%%json saveResponseAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="profileDataFrame">
                    <value>
                        <![CDATA[
val dfProf =
com.thinkbiganalytics.spark.dataprofiler.core.Profiler.profileDataFrame(sparkContextService,profiler,df)
%%json dfProf
                        ]]>
                    </value>
                </entry>


                <entry key="pagedDataFrame">
                    <value>
                        <![CDATA[
var dfRows : List[Object] = List()
LivyLogger.time {
  val (startCol, stopCol) = (%s, %s);
  val lastCol = df.columns.length - 1
  val dfStartCol = if( lastCol >= startCol ) startCol else lastCol
  val dfStopCol = if( lastCol >= stopCol) stopCol else lastCol
  df = df.select( dfStartCol to dfStopCol map df.columns map col: _*)

  val dl = df.collect
  val ( firstRow, lastRow ) = ( 0, dl.size )
  val ( pageStart, pageStop ) = ( %s, %s )
  val dfStartRow = if( lastRow >= pageStart ) pageStart else lastRow
  val dfStopRow = if( lastRow >= pageStop) pageStop else lastRow

  val pagedRows = dl.slice( dfStartRow, dfStopRow ).map( _.toSeq )
  dfRows = List( df.schema.json, pagedRows )
}
                        ]]>
                    </value>
                </entry>

                <entry key="getDataSources">
                    <value>
                        <![CDATA[
val dataSources = sparkUtilityService.getDataSources()

val dataSourcesAsStr = mapper.writeValueAsString(dataSources)
%%json dataSourcesAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="getSaveResult">
                    <value>
                        <![CDATA[
val saveResult = sparkShellTransformController.getSaveResult(%s)
val saveResultAsStr = mapper.writeValueAsString(saveResult.getPath().toUri())
%%json saveResultAsStr
                        ]]>
                    </value>
                </entry>
                <entry key="kyloCatalogTransform">
                    <value> <!-- takes a JSON string escaped for scala -->
                        <![CDATA[
val kyloCatalogReadRequest: com.thinkbiganalytics.spark.rest.model.KyloCatalogReadRequest = mapper.readValue(%s, classOf[com.thinkbiganalytics.spark.rest.model.KyloCatalogReadRequest]);
val kyloReadResponse = transformService.kyloReaderResponse(kyloCatalogReadRequest);
val transformAsStr = mapper.writeValueAsString(kyloReadResponse)
%%json transformAsStr
]]>
                    </value>
                </entry>
                <entry key="getTransform">
                    <value> <!-- takes a JSON string escaped for scala -->
                        <![CDATA[
val response = sparkShellTransformController.getTableTransformResponse(%s)
val transformAsStr = mapper.writeValueAsString(response)
%%json transformAsStr
]]>
                    </value>
                </entry>
            </map>
        </constructor-arg>
    </bean>

</beans>