###
# #%L
# thinkbig-service-app
# %%
# Copyright (C) 2017 ThinkBig Analytics
# %%
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# #L%
###
###
# Other properties are in the services-app/application.properties
#
# Add extra profiles by setting "spring.profiles.active=<profile-name>" property on command line, e.g.
# -Dspring.profiles.active=hdp24,gmail,cdh
# Extra profiles will add to this set of profiles and override properties given in this file
spring.profiles.include=native,nifi-v1,auth-kylo,auth-file



# Spring Datasource properties for spring batch and the default data source
# NOTE: Cloudera default password for root access to mysql is "cloudera"
#
spring.datasource.url=jdbc:mysql://localhost:3306/kylo
spring.datasource.username=root
spring.datasource.password=hadoop
spring.datasource.maxActive=30
spring.datasource.validationQuery=SELECT 1
spring.datasource.testOnBorrow=true
spring.datasource.driverClassName=org.mariadb.jdbc.Driver
spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect
spring.jpa.open-in-view=true
#
#Postgres datasource configuration
#
#spring.datasource.url=jdbc:postgresql://localhost:5432/pipeline_db
#spring.datasource.driverClassName=org.postgresql.Driver
#spring.datasource.username=root
#spring.datasource.password=thinkbig
#spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect

###
# Authentication settings:
#
# Currently available athentication/authorization Spring profiles:
#   *  auth-kylo   - Users are authenticated if they exist in the Kylo user store and any
#                    groups associated with the user are retrieved for access control.
#                    This profile is usually used in conjuction with other auth profiles
#                    (auth-ldap, auth-ad, etc.)
#   *  auth-ldap   - authenticates users using LDAP and optionally loads any associated
#                    user groups
#   *  auth-ad     - authenticates users using Active Directory and loads any associated
#                    user groups
#   *  auth-simple - Uses authenticationService.username and authenticationService.password
#                    for authentication (development only)
#   *  auth-file   - Uses users.properties and roles.properties files for authentication
#                    and role assignment (generally for development only)

## auth-file: If this profile is active then these optional properties may be used:

#security.auth.file.users=file:///opt/kylo/users.properties
#security.auth.file.groups=file:///opt/kylo/groups.properties

## auth-simple: If this profile is active these authenticationService properties are used:

#authenticationService.username=dladmin
#authenticationService.password={cipher}52fd39e4e4f7d0f6a91989efbfa870f1a543550401e6ab0b17f3059c1ada9b5f
#authenticationService.password=thinkbig

## auth-ldap: If this profile is active then these properties should uncommented and updated appropriately

#security.auth.ldap.server.uri=ldap://localhost:52389/dc=example,dc=com
#security.auth.ldap.server.authDn=uid=dladmin,ou=people,dc=example,dc=com
#security.auth.ldap.server.password=thinkbig
### user DN patterns are separated by '|'
#security.auth.ldap.authenticator.userDnPatterns=uid={0},ou=people
#security.auth.ldap.user.enableGroups=true
#security.auth.ldap.user.groupNameAttr=ou
#security.auth.ldap.user.groupsBase=ou=groups

## auth-ad: If this profile is active then these properties should uncommented and updated appropriately

#security.auth.ad.server.uri=ldap://example.com/
#security.auth.ad.server.domain=test.example.com
#security.auth.ad.user.enableGroups=true
## group attribute patterns are separated by '|'
#security.auth.ad.user.groupAttributes=



###Ambari Services Check
ambariRestClientConfig.host=127.0.0.1
ambariRestClientConfig.port=8080
ambariRestClientConfig.username=admin
ambariRestClientConfig.password=admin
#ambariRestClientConfig.serverUrl=http://127.0.0.1:8080/api/v1
ambari.services.status=HDFS,HIVE,MAPREDUCE2,SQOOP

###Cloudera Services Check
#clouderaRestClientConfig.username=cloudera
#clouderaRestClientConfig.password=cloudera
#clouderaRestClientConfig.serverUrl=127.0.0.1
#cloudera.services.status=
##HDFS/[DATANODE,NAMENODE,SECONDARYNAMENODE],HIVE/[HIVEMETASTORE,HIVESERVER2],YARN,SQOOP

#
# Server port
#
server.port=8420
# server.port=8443
##### ADD IF SSL is needed
###
#server.ssl.key-store=/Users/sr186054/tools/test-ssl/test/localhost/keystore.jks
#server.ssl.key-store-password=sxkJ96yw2ZZktkVFtflln2IqjxkXPCD+vh3gAPDhQ18
#server.ssl.key-store-type=jks
#server.ssl.trust-store=/Users/sr186054/tools/test-ssl/test/localhost/truststore.jks
#server.ssl.trust-store-password=S1+cc2FKMzk2td/p6OJE0U6FUM3fV5jnlrYj46CoUSU
#server.ssl.trust-store-type=JKS

#
# General configuration - Note: Supported configurations include STANDALONE, BUFFER_NODE_ONLY, BUFFER_NODE, EDGE_NODE
#
application.mode=STANDALONE

#
# Turn on debug mode to display more verbose error messages in the UI
#
application.debug=true

#
# Prevents execution of jobs at startup.  Change to true, and the name of the job that should
# be run at startup if we want that behavior
#
spring.batch.job.enabled=false
spring.batch.job.names=


#spring.jpa.show-sql=true
#spring.jpa.hibernate.ddl-auto=validate

# NOTE: For Cloudera metadata.datasource.password=cloudera is required
metadata.datasource.driverClassName=${spring.datasource.driverClassName}
metadata.datasource.url=${spring.datasource.url}
metadata.datasource.username=${spring.datasource.username}
metadata.datasource.password=${spring.datasource.password}
metadata.datasource.validationQuery=SELECT 1
metadata.datasource.testOnBorrow=true


# NOTE: For Cloudera hive.datasource.username=hive is required
hive.userImpersonation.enabled=false
hive.datasource.driverClassName=org.apache.hive.jdbc.HiveDriver
hive.datasource.url=jdbc:hive2://localhost:10000/default
hive.datasource.username=hive
hive.datasource.password=hive
hive.datasource.validationQuery=show tables 'test'


# NOTE: For Cloudera hive.metastore.datasource.password=cloudera is required
##Also Clouder  url should be /metastore instead of /hive
hive.metastore.datasource.driverClassName=org.mariadb.jdbc.Driver
hive.metastore.datasource.url=jdbc:mysql://localhost:3306/hive
#hive.metastore.datasource.url=jdbc:mysql://localhost:3306/metastore
hive.metastore.datasource.username=root
hive.metastore.datasource.password=hadoop
hive.metastore.datasource.validationQuery=SELECT 1
hive.metastore.datasource.testOnBorrow=true

modeshape.datasource.driverClassName=${spring.datasource.driverClassName}
modeshape.datasource.url=${spring.datasource.url}
modeshape.datasource.username=${spring.datasource.username}
modeshape.datasource.password=${spring.datasource.password}


nifi.rest.host=localhost
nifi.rest.port=8079

###
# NiFi Https configuration below
#
# Follow directions here: https://wiki.thinkbiganalytics.com/x/twHK  to setup certificates and properties in NiFi
#
### The port should match the port found in the /opt/nifi/current/conf/nifi.properties (nifi.web.https.port)
#nifi.rest.port=9443
#nifi.rest.https=true
#nifi.rest.useConnectionPooling=false
#nifi.rest.truststorePath=/opt/nifi/data/ssl/localhost/truststore.jks
#####the truststore password below needs to match that found in the nifi.properties file (nifi.security.truststorePasswd)
#nifi.rest.truststorePassword=Zl1mAbMm0v4UkGV8VYjTi2ZP8NdwUL9CW7nsjGo47Fs
#nifi.rest.truststoreType=JKS
#nifi.rest.keystorePath=/opt/nifi/data/ssl/CN=kylo_OU=NIFI.p12
###value found in the .password file /opt/nifi/data/ssl/CN=kylo_OU=NIFI.password
#nifi.rest.keystorePassword=ydPkkba
#nifi.rest.keystoreType=PKCS12
#




elasticsearch.host=localhost
elasticsearch.port=9300
elasticsearch.clustername=demo-cluster

kerberos.hive.kerberosEnabled=false
#kerberos.hive.hadoopConfigurationResources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
#kerberos.hive.kerberosPrincipal=hive/sandbox.hortonworks.com
#kerberos.hive.keytabLocation=/etc/security/keytabs/hive-thinkbig.headless.keytab

## used to map Nifi Controller Service connections to the User Interface
## naming convention for the property is nifi.service.NIFI_CONTROLLER_SERVICE_NAME.NIFI_PROPERTY_NAME
##anything prefixed with nifi.service  will be used by the UI.  Replace Spaces with underscores and make it lowercase.
nifi.service.mysql.database_user=root
nifi.service.mysql.password=hadoop

nifi.service.hive_thrift_service.database_connection_url=jdbc:hive2://localhost:10000/default
#nifi.service.hive_thrift_service.kerberos_principal=nifi
#nifi.service.hive_thrift_service.kerberos_keytab=/etc/security/keytabs/nifi.headless.keytab
#nifi.service.hive_thrift_service.hadoop_configuration_resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml

nifi.service.kylo_metadata_service.rest_client_url=http://localhost:8400/proxy/v1/metadata
nifi.service.kylo_metadata_service.rest_client_password=thinkbig

jms.client.id=thinkbig.feedmgr
jms.activemq.broker.url=tcp://localhost:61616
#jms.activemq.broker.username=admin
#jms.activemq.broker.password=admin

## nifi Property override with static defaults
##Static property override supports 3 usecases
# 1) store properties in the file starting with the prefix defined in the "PropertyExpressionResolver class"  default = config.
# 2) store properties in the file starting with "nifi.<PROCESSORTYPE>.<PROPERTY_KEY>   where PROCESSORTYPE and PROPERTY_KEY are all lowercase and the spaces are substituted with underscore
# 3) Global property replacement.  properties starting with "nifi.all_processors.<PROPERTY_KEY> will globally replace the value when the template is instantiated
##Below are Ambari configuration options for Hive Metastore and Spark location
config.hive.schema=hive
config.metadata.url=http://localhost:8400/proxy/v1/metadata

## Spark configuration
nifi.executesparkjob.sparkhome=/usr/hdp/current/spark-client
nifi.executesparkjob.sparkmaster=local
nifi.executesparkjob.driver_memory=1024m
nifi.executesparkjob.number_of_executors=1
nifi.executesparkjob.executor_cores=1
config.spark.validateAndSplitRecords.extraJars=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar

## Specify to override the default HDFS locations for feed tables and multi-tenancy.

# Root HDFS locations for new raw files
config.hdfs.ingest.root=/etl

# Root HDFS location for Hive ingest processing tables (raw,valid,invalid)
config.hive.ingest.root=/model.db
# Root HDFS location for Hive profile table
config.hive.profile.root=/model.db
# Root HDFS location for Hive master table
config.hive.master.root=/app/warehouse

# Prefix to prepend to category system name for this environment (blank if none). Use for multi-tenancy
config.category.system.prefix=

# Set the JMS server hostname for the Kylo hosted JMS server
config.elasticsearch.jms.url=tcp://localhost:61616


#example of replacing global properties
#nifi.all_processors.kerberos_principal=nifi
#nifi.all_processors.kerberos_keytab=/etc/security/keytabs/nifi.headless.keytab
#nifi.all_processors.hadoop_configuration_resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml


##cloudera config
#config.hive.schema=metastore
#nifi.executesparkjob.sparkhome=/usr/lib/spark


## how often should SLAs be checked
sla.cron.default=0 0/5 * 1/1 * ? *

# Additional Hive UDFs for partition functions. Separate multiple functions with commas.
#kylo.metadata.udfs=

### Sqoop import configuration
# DB Connection password and driver (format: nifi.service.<sqoop controller service name in NiFi>.<key>=<value>
# Note: Ensure that the driver jar is available in below two locations:
# (a) sqoop's lib directory (e.g. /usr/hdp/current/sqoop-client/lib/)
# (b) kylo's lib directory, and owned by 'kylo' user (/opt/kylo/kylo-services/lib)
#nifi.service.sqoop-mysql-connection.password=hadoop
#nifi.service.sqoop-mysql-connection.database_driver_class_name=com.mysql.jdbc.Driver
# Base HDFS landing directory
#config.sqoop.hdfs.ingest.root=/sqoopimport


##uncommenet the settings below for Gmail to work
#sla.mail.protocol=smtp
#sla.mail.host=smtp.google.com
#sla.mail.port=587
#sla.mail.smtpAuth=true
#sla.mail.starttls=true

# Login form authentication
#security.jwt.algorithm=HS256
security.jwt.key=<insert-256-bit-secret-key-here>
#security.rememberme.alwaysRemember=false
#security.rememberme.cookieDomain=localhost
#security.rememberme.cookieName=remember-me
#security.rememberme.parameter=remember-me
#security.rememberme.tokenValiditySeconds=1209600
#security.rememberme.useSecureCookie=
## if a job fails tell operations manager to query nifi for bulletin information in an attempt to capture more logs about the failure
kylo.ops.mgr.query.nifi.bulletins=true
